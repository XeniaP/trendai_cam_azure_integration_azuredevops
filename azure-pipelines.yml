trigger:
- main

variables:
  AZURE_SERVICE_CONNECTION: 'AutomationCAM'
  SELF_HOSTED_POOL_NAME: 'CAMAutomation' # Optional
  MAX_PARALLEL: 3
  TF_VERSION: '1.14.1'
  #MAIN_REGION: 'eastus'

stages:
- stage: deploy
  displayName: Deploy in parallel per subscription
  jobs:

  - job: build_matrix
    workspace:
      clean: all
    displayName: Build matrix from Azure subscriptions
    pool:
      name: $(SELF_HOSTED_POOL_NAME)
      #vmImage: ubuntu-latest
    steps:
    - checkout: self
    - bash: |
        set -euo pipefail
        python3 - <<'PY'
        import json
        subs = json.load(open("subscriptions/subs.json"))
        matrix = {}
        for s in subs:
          key = (
              s["name"]
              .replace(" ", "_")
              .replace("/", "_")
              .replace("(", "")
              .replace(")", "")
          )
          matrix[key] = {
              "SUB_NAME": s["name"],
              "SUB_ID": s["id"],
              "BACKEND_URL": s["backend_url"],
              "STORAGE_ACCOUNT": s["storage_account"],
              "MAIN_REGION": s["main_region"]
          }
        print("##vso[task.setvariable variable=MATRIX;isOutput=true]" + json.dumps(matrix))
        PY
      name: setMatrix
      displayName: "Create matrix output variable"

    - publish: subscriptions/subs.json
      artifact: subs_json
      displayName: "Publish subs.json (debug)"

  - job: run_per_subscription
    displayName: Run per subscription (parallel)
    dependsOn: build_matrix
    pool:
      name: $(SELF_HOSTED_POOL_NAME)
      #vmImage: ubuntu-latest
    workspace:
      clean: all
    strategy:
      matrix: $[ dependencies.build_matrix.outputs['setMatrix.MATRIX'] ]
      maxParallel: ${{ variables.MAX_PARALLEL }}

    steps:
    - checkout: self

    - bash: |
        #!/usr/bin/env bash
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive

        # Base deps
        sudo apt-get update -y
        sudo apt-get install -y --no-install-recommends ca-certificates curl unzip jq python3 python3-venv python3-pip
        # --- Install Azure CLI in a dedicated venv (no apt repo Microsoft) ---
        if ! command -v az >/dev/null 2>&1; then
          sudo rm -rf /var/lib/apt/lists/* && sudo apt-get update && sudo curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
        fi
        az --version | head -n 2
        az config set extension.use_dynamic_install=yes_without_prompt
      displayName: "Install Azure CLI"
    
    - bash: |
        #!/usr/bin/env bash
        set -euo pipefail
        rm -rf .terraform*
        if command -v terraform >/dev/null 2>&1; then
          CUR="$(terraform version | head -n1 | awk '{print $2}' | tr -d 'v')"
        else
          CUR=""
        fi
        if [[ "$CUR" != "$TF_VERSION" ]]; then
          echo "== Install Terraform ${TF_VERSION} =="
          TF_DIR="$(Agent.TempDirectory)/terraform"
          mkdir -p "$TF_DIR"
          curl -fsSLo /tmp/terraform.zip \
            "https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip"
          unzip -o /tmp/terraform.zip -d "$TF_DIR"
          chmod +x "$TF_DIR/terraform"
          sudo ln -sf "$TF_DIR/terraform" /usr/local/bin/terraform && export PATH="$TF_DIR:$PATH"
          echo "##vso[task.prependpath]$TF_DIR"
        else
          echo "Terraform already at $TF_VERSION"
        fi
      displayName: "Install Terraform and dependencies"
    
    - task: AzureCLI@2
      displayName: "Check Quota and permissions"
      name: CheckQuota
      inputs:
        azureSubscription: $(AZURE_SERVICE_CONNECTION)
        scriptType: 'bash'
        scriptLocation: inlineScript
        addSpnToEnvironment: true
        inlineScript: |
          set -euo pipefail
          #!/bin/bash
          set -euo pipefail

          SUB_ID=$(SUB_ID)
          MAIN_REGION=$(MAIN_REGION)

          # Listas de regiones permitidas (como strings para comparaciones rápidas en Bash)
          ALLOWED_AVTD_LIST="southafricanorth australiaeast australiasoutheast centralindia eastasia japaneast japanwest koreacentral southindia southeastasia westindia canadacentral canadaeast francecentral germanywestcentral northeurope norwayeast swedencentral switzerlandnorth uksouth ukwest westeurope uaenorth brazilsouth centralus eastus eastus2 northcentralus southcentralus westus westus2 westus3"
          ALLOWED_DSPM_LIST="australiaeast centralindia japaneast southeastasia germanywestcentral uksouth uaenorth brazilsouth eastus"

          # --- Funciones de Cuota ---
          get_y1_limit() {
              local sub="$1" reg="$2"
              local out
              out=$(az rest --method get --url "https://management.azure.com/subscriptions/$sub/providers/Microsoft.Web/locations/$reg/usages?api-version=2024-11-01" --query "value[?contains(name.value,'dynamic') && contains(name.value,'Linux')].limit | [0]" -o tsv 2>/dev/null | tr -d '\r' || echo "0")
              [[ "$out" =~ ^[0-9]+$ ]] && echo "$out" || echo "0"
          }

          get_basv2_cores_limit() {
              local sub="$1" reg="$2"
              local out
              out=$(az rest --method get --url "https://management.azure.com/subscriptions/$sub/providers/Microsoft.Compute/locations/$reg/usages?api-version=2023-07-01" --query "value[?name.value=='standardBSv2Family'].limit | [0]" -o tsv 2>/dev/null | tr -d '\r' || echo "0")
              [[ "$out" =~ ^[0-9]+$ ]] && echo "$out" || echo "0"
          }

          # 1. Chequeo de EP1 en Región Principal
          EP1_AVAILABLE=$(az rest --method get --url "https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.Web/locations/$MAIN_REGION/usages?api-version=2024-11-01" --query "value[?name.value=='EP1'] | [0] | currentValue < limit" -o tsv 2>/dev/null || echo "false")
          #FS_ENABLE=$([ "$EP1_AVAILABLE" == "true" ] && echo "true" || echo "false")
          FS_ENABLE="true"

          RAW_AVTD=$(az graph query -q "resources | where subscriptionId == '$SUB_ID' | where type in~ ('microsoft.compute/virtualmachines', 'microsoft.containerregistry/registries') | project location | distinct location" --query "data[].location" -o tsv | tr '[:upper:]' '[:lower:]')

          FINAL_AVTD_ARRAY=()
          for REG in $RAW_AVTD; do
              # Solo si la región está en la lista permitida
              if [[ $ALLOWED_AVTD_LIST =~ (^|[[:space:]])$REG($|[[:space:]]) ]]; then
                FINAL_AVTD_ARRAY+=("$REG")
              fi
          done

          echo "Procesando regiones DSPM..."
          RAW_DSPM=$(az graph query -q "resources | where subscriptionId == '$SUB_ID' | where type =~ 'microsoft.compute/virtualmachines' or type =~ 'microsoft.storage/storageaccounts' | project location | distinct location" --query "data[].location" -o tsv | tr '[:upper:]' '[:lower:]')

          FINAL_DSPM_ARRAY=()
          for REG in $RAW_DSPM; do
              # Solo si la región está en la lista permitida
              if [[ $ALLOWED_DSPM_LIST =~ (^|[[:space:]])$REG($|[[:space:]]) ]]; then
                FINAL_DSPM_ARRAY+=("$REG")
              fi
          done
          AVTD_JSON=$(printf '%s\n' "${FINAL_AVTD_ARRAY[@]:-}" | jq -R . | jq -s -c 'map(select(. != ""))')
          DSPM_JSON=$(printf '%s\n' "${FINAL_DSPM_ARRAY[@]:-}" | jq -R . | jq -s -c 'map(select(. != ""))')

          echo "AVTD: $AVTD_JSON"
          echo "DSPM: $DSPM_JSON"

          echo "##vso[task.setvariable variable=FS_ENABLE;isOutput=true]$FS_ENABLE"
          echo "##vso[task.setvariable variable=DEPLOY_AVTD_REGIONS;isOutput=true]$AVTD_JSON"
          echo "##vso[task.setvariable variable=DEPLOY_DSPM_REGIONS;isOutput=true]$DSPM_JSON"
      env:
        SUB_ID: $(SUB_ID)
        REGION: $(MAIN_REGION)

    - task: AzureCLI@2
      displayName: "Deploy Integration"
      inputs:
        azureSubscription: $(AZURE_SERVICE_CONNECTION)
        scriptType: 'bash'
        scriptLocation: inlineScript
        addSpnToEnvironment: true
        inlineScript: |
          set -euo pipefail
          
          # Capturamos los valores (que vienen como [reg1,reg2])
          RAW_AVTD=$(CheckQuota.DEPLOY_AVTD_REGIONS)
          RAW_DSPM=$(CheckQuota.DEPLOY_DSPM_REGIONS)

          # FUNCIÓN MÁGICA: Convierte el texto [a,b] en JSON real ["a","b"]
          to_clean_json() {
            local input="$1"
            echo "$input" | tr -d '[]' | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R . | jq -s -c .
          }

          # Re-procesamos para asegurar que tengan comillas
          export AVTD_REGIONS=$(to_clean_json "$RAW_AVTD")
          export DSPM_REGIONS=$(to_clean_json "$RAW_DSPM")
          export FS_STATUS=$(CheckQuota.FS_ENABLE)

          echo "FS_STATUS: $FS_STATUS"
          echo "AVTD_REGIONS: $AVTD_REGIONS" # Ahora saldrá con ["..."]
          echo "DSPM_REGIONS: $DSPM_REGIONS"

          echo "== Setting TrendAI Environment =="
          export PKG_ZIP="cloud-account-management-terraform-package.zip"
          export PKG_DIR="cloud-account-management-terraform-package"

          rm -rf "$PKG_DIR"
          mkdir -p "$PKG_DIR"
          curl -fsSL \
            "$BACKEND_URL" \
            -o "$PKG_ZIP"
          
          unzip -o "$PKG_ZIP" -d "$PKG_DIR"

          function remove_feature_module_block() {
              local feature=$1
              if [ -z "$feature" ]; then
                  echo "[ERROR] Feature name is required"
                  return 1
              fi

              awk -v feature="$feature" '
                  BEGIN { found=0; depth=0 }
                  /module[[:space:]]*"'"$feature"'"[[:space:]]*{/ { found=1; depth=1; next }
                  found && /{/ { depth++ }
                  found && /}/ { depth--; if (depth == 0) { found=0; next } }
                  !found { print }
                  ' main.tf > main.tf.tmp && mv main.tf.tmp main.tf
          }

          echo "== Override Configuration according environment characteristics =="
          find "$PKG_DIR" -maxdepth 2 -type f -name "*.sh" -print
          chmod +x "$PKG_DIR"/*.sh || true
          chmod +x "$PKG_DIR"/cam/*.sh 2>/dev/null || true
          sed -i "s/export CAM_DEPLOYED_REGION=\".*\"/export CAM_DEPLOYED_REGION=\"${MAIN_REGION}\"/" "$PKG_DIR/deploy.sh" || true
          sed -i "s|^export SUBSCRIPTION_ID=.*|export SUBSCRIPTION_ID=\"${SUB_ID}\"|g" "$PKG_DIR/deploy.sh" || true
          sed -i "s|^export CLOUD_ACCOUNT_NAME=.*|export CLOUD_ACCOUNT_NAME=\"${CLOUD_ACCOUNT_NAME}\"|g" "$PKG_DIR/deploy.sh" || true
          sed -i "s|^export TF_AUTO_APPROVE=.*|export TF_AUTO_APPROVE=true|g" "$PKG_DIR/deploy.sh" || true
          sed -i 's/default *= *true/default = false/g' "$PKG_DIR/cam/variables.tf"

          echo "== Setting custom configuration for Data Security Posture Management =="
          if [[ -z "$DSPM_REGIONS" || "$DSPM_REGIONS" == "[]" ]]; then
            REGIONS_COUNT_DSPM=0
            sed -i 's/"data-security-posture-management": .*/"data-security-posture-management": [],/' "$PKG_DIR/deploy.sh"
            DSPM_JSON="[]"
          else
            echo "DSPM will be deployed in the following regions: $DSPM_REGIONS"
            DSPM_JSON=$(echo "$DSPM_REGIONS" | jq -c .)
          fi

          echo "== Setting custom configuration for Agentless Vulnerability & Threat Detection =="
          if [[ -z "$AVTD_REGIONS" || "$AVTD_REGIONS" == "[]" ]]; then
              REGIONS_COUNT_AVTD=0
              sed -i 's/"cloud-sentry": .*/"cloud-sentry": [],/' "$PKG_DIR/deploy.sh"
              AVTD_JSON="[]"
          else
              echo "AVTD will be deployed in the following regions: $AVTD_REGIONS"
              AVTD_JSON=$(echo "$AVTD_REGIONS" | jq -c .)
          fi
          NEW_JSON_VALUE=$(jq -n -c \
            --argjson cs "$AVTD_JSON" \
            --argjson dspm "$DSPM_JSON" \
            '{
              "cloud-sentry": $cs,
              "data-security-posture-management": $dspm,
              "file-storage-security": null,
              "real-time-posture-monitoring": null
            }')
          echo "New FEATURES_DEPLOYED_REGIONS value: $NEW_JSON_VALUE"
          perl -i -0777 -pe "s/export FEATURES_DEPLOYED_REGIONS='\{.*?\}'/export FEATURES_DEPLOYED_REGIONS='$NEW_JSON_VALUE'/s" "$PKG_DIR/deploy.sh"

          echo "== Setting custom configuration for File Storage Security =="
          if [[ "$FS_STATUS" == "false" ]]; then
            echo "== File Storage Security will be disabled due to lack of EP1 quota in the main region =="
            remove_feature_module_block "file-storage-security"
            sed -i '/"file-storage-security"[[:space:]]*:[[:space:]]*null,/d' "$PKG_DIR/deploy.sh"
          else
            echo "== Additional Configuration for File Storage Security =="
            if [[ -n "${STORAGE_ACCOUNT:-}" ]]; then
              echo "Setting quarantine storage account to ${STORAGE_ACCOUNT}"
              sed -i "s|quarantine_storage_account *= *\"[^\"]*\"|quarantine_storage_account = \"${STORAGE_ACCOUNT}\"|g" "$PKG_DIR/main.tf" || true
            fi
          fi

          echo "== DEBUG DEPLOY.SH =="
          cat "$PKG_DIR/deploy.sh"  # Debug: Check deploy.sh content after modifications

          echo "== Setting the subscription context =="
          az account set --subscription "$SUB_ID"

          echo "== Setting authentication parameters =="
          export ARM_SUBSCRIPTION_ID="$SUB_ID"
          export ARM_TENANT_ID="$tenantId"
          export ARM_CLIENT_ID="$servicePrincipalId"
          if [[ -n "${servicePrincipalKey:-}" ]]; then
            echo "== Using SPN secret auth =="
            export ARM_CLIENT_SECRET="$servicePrincipalKey"
            unset ARM_USE_OIDC ARM_OIDC_TOKEN || true
          elif [[ -n "${idToken:-}" ]]; then
            echo "== Using OIDC auth (federated service connection) =="
            export ARM_USE_OIDC=true
            export ARM_OIDC_TOKEN="$idToken"
            unset ARM_CLIENT_SECRET || true
          else
            echo "ERROR: No servicePrincipalKey and no idToken. Check your Service Connection type." >&2
            exit 1
          fi

          echo "== Deploying with Terraform =="
          cd "$PKG_DIR"
          export TF_LOG=ERROR
          ./deploy.sh
      env:
        SUB_ID: $(SUB_ID)
        BACKEND_URL: $(BACKEND_URL)
        STORAGE_ACCOUNT: $(STORAGE_ACCOUNT)
        CLOUD_ACCOUNT_NAME: "$(SUB_NAME)"
        MAIN_REGION: $(MAIN_REGION)
